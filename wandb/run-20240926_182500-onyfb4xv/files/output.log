  0%|                                                                                                                                                  | 0/40 [00:00<?, ?it/s]/home/user/anaconda3/envs/ch_llm/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/user/anaconda3/envs/ch_llm/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]









 25%|██████████████████████████████████                                                                                                      | 10/40 [17:10<51:24, 102.82s/it]









 48%|███████████████████████████████████████████████████████████████████████▋                                                                               | 19/40 [31:58<32:02, 91.55s/it]










 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                         | 29/40 [39:44<08:46, 47.89s/it]










100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [48:17<00:00, 72.43s/it]
/home/user/anaconda3/envs/ch_llm/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
{'loss': 11303.3836, 'grad_norm': 0.6694786548614502, 'learning_rate': 0.0002, 'epoch': 4.9}
{'train_runtime': 2900.2221, 'train_samples_per_second': 1.796, 'train_steps_per_second': 0.014, 'train_loss': 7745.14775390625, 'epoch': 4.9}
 If there's a warning about missing keys above, please disregard :)
Training LLaMA2 model with params:
base_model: meta-llama/Llama-2-7b-chat-hf
data_path: /home/user/chaehee/llama_project/data/train_dataset.json
output_dir: ./output
batch_size: 128
micro_batch_size: 4
num_epochs: 5
learning_rate: 0.0005
cutoff_len: 256
val_set_size: 2000
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj']
train_on_inputs: False
add_eos_token: False
group_by_length: False
wandb_project:
wandb_run_name:
wandb_watch:
wandb_log_model:
resume_from_checkpoint: False
prompt template: prompt
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.16s/it]
/home/user/anaconda3/envs/ch_llm/lib/python3.8/site-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
Map:   5%|███████                                                                                                                                 | 54/1042 [00:00<00:01, 532.09 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1042/1042 [00:02<00:00, 470.81 examples/s]


Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 1998/2000 [00:04<00:00, 497.66 examples/s]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:04<00:00, 492.37 examples/s]
/home/user/anaconda3/envs/ch_llm/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead:
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/user/anaconda3/envs/ch_llm/lib/python3.8/site-packages/accelerate/accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = torch.cuda.amp.GradScaler(**kwargs)










 25%|█████████████████████████████████████▊                                                                                                                 | 10/40 [07:46<23:17, 46.57s/it]










 50%|███████████████████████████████████████████████████████████████████████████▌                                                                           | 20/40 [15:32<15:32, 46.64s/it]










 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 30/40 [23:19<07:46, 46.69s/it]










100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [31:05<00:00, 46.64s/it]
{'loss': 10483.9688, 'grad_norm': 1.033820390701294, 'learning_rate': 0.0002, 'epoch': 4.9}
{'train_runtime': 1865.5535, 'train_samples_per_second': 2.793, 'train_steps_per_second': 0.021, 'train_loss': 17054.12890625, 'epoch': 4.9}
 If there's a warning about missing keys above, please disregard :)